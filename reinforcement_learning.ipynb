{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpi8YcxNvdN3nMwBz/9EGt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uma29-10/TNSIF_AIML_DBIT/blob/main/reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qtq6sTP175IM",
        "outputId": "2d6b4f45-5292-4bfd-e953-b6e4d2799dcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged in 6 iterations\n",
            "\n",
            "Value function (rows top->bottom):\n",
            " -4.10  -3.44  -2.71  -1.90 \n",
            " -3.44  -2.71  -1.90  -1.00 \n",
            " -2.71  -1.90  -1.00   0.00 \n",
            " -1.90  -1.00   0.00   0.00 \n",
            "\n",
            "Policy (T=terminal):\n",
            " v  v  v  v \n",
            " v  v  v  v \n",
            " v  v  v  v \n",
            " >  >  >  T \n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "ROWS, COLS = 4, 4\n",
        "TERMINAL = (3, 3)\n",
        "\n",
        "ACTIONS = {\n",
        "    'U': (-1, 0),\n",
        "    'D': (1, 0),\n",
        "    'L': (0, -1),\n",
        "    'R': (0, 1)\n",
        "}\n",
        "\n",
        "GAMMA = 0.9\n",
        "THETA = 1e-4\n",
        "STEP_REWARD = -1\n",
        "TERMINAL_REWARD = 0\n",
        "\n",
        "# All states as (r, c) tuples\n",
        "states = [(r, c) for r in range(ROWS) for c in range(COLS)]\n",
        "\n",
        "def is_terminal(s):\n",
        "    return s == TERMINAL\n",
        "\n",
        "def next_state(s, action):\n",
        "    \"\"\"Deterministic transition with wall bounce (stay if would go off-grid).\"\"\"\n",
        "    if is_terminal(s):\n",
        "        return s\n",
        "    dr, dc = ACTIONS[action]\n",
        "    nr = max(0, min(ROWS - 1, s[0] + dr))\n",
        "    nc = max(0, min(COLS - 1, s[1] + dc))\n",
        "    return (nr, nc)\n",
        "\n",
        "def reward(s, s2):\n",
        "    \"\"\"Reward for moving from s to s2.\"\"\"\n",
        "    return TERMINAL_REWARD if is_terminal(s2) else STEP_REWARD\n",
        "\n",
        "# Initialize values to zero\n",
        "V = {s: 0.0 for s in states}\n",
        "\n",
        "# Value Iteration\n",
        "iteration = 0\n",
        "while True:\n",
        "    iteration += 1\n",
        "    delta = 0.0\n",
        "    newV = V.copy()  # synchronous update\n",
        "\n",
        "    for s in states:\n",
        "        if is_terminal(s):\n",
        "            continue\n",
        "        # compute value for each action (deterministic => single next state)\n",
        "        action_values = []\n",
        "        for a in ACTIONS:\n",
        "            s2 = next_state(s, a)\n",
        "            r = reward(s, s2)\n",
        "            q = r + GAMMA * V[s2]\n",
        "            action_values.append(q)\n",
        "\n",
        "        best = max(action_values)\n",
        "        newV[s] = best\n",
        "        delta = max(delta, abs(V[s] - newV[s]))\n",
        "\n",
        "    V = newV\n",
        "    if delta < THETA:\n",
        "        break\n",
        "\n",
        "# Extract greedy policy\n",
        "policy = {}\n",
        "arrow = {'U': '^', 'D': 'v', 'L': '<', 'R': '>'}\n",
        "\n",
        "for s in states:\n",
        "    if is_terminal(s):\n",
        "        policy[s] = 'T'\n",
        "        continue\n",
        "    best_a = None\n",
        "    best_q = float('-inf')\n",
        "    for a in ACTIONS:\n",
        "        s2 = next_state(s, a)\n",
        "        q = reward(s, s2) + GAMMA * V[s2]\n",
        "        if q > best_q:\n",
        "            best_q = q\n",
        "            best_a = a\n",
        "    policy[s] = arrow[best_a]\n",
        "\n",
        "# Print values and policy in grid form\n",
        "print(\"Converged in\", iteration, \"iterations\\n\")\n",
        "print(\"Value function (rows top->bottom):\")\n",
        "for r in range(ROWS):\n",
        "    for c in range(COLS):\n",
        "        print(f\"{V[(r,c)]:6.2f}\", end=\" \")\n",
        "    print()\n",
        "\n",
        "print(\"\\nPolicy (T=terminal):\")\n",
        "for r in range(ROWS):\n",
        "    for c in range(COLS):\n",
        "        print(f\" {policy[(r,c)]}\", end=\" \")\n",
        "    print()"
      ]
    }
  ]
}